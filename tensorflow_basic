import tensorflow as tf
tf.__version__
# Tensorflor 常用损失函数
cost = tf.square(Y-y_model) # 

# <增加GPU训练模块>
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # CUDA_VISIBLE_DEVICES=0,2,3       Devices 0, 2, 3 will be visible; device 1 is masked
                                         # CUDA_VISIBLE_DEVICES=""          No GPU will be visible
sess_config = tf.ConfigProto() 
sess_config.gpu_options.per_process_gpu_memory_fraction = 0.70 
sess = tf.Session(config=sess_config) 

# <tensorboard 训练可视化>

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    #选定可视化存储目录    
    writer = tf.summary.FileWriter("./train-log",sess.graph)   
    test_writer = tf.summary.FileWriter("./train-log") 
    print(sess.run(add))
writer.close()
tensorboard --logdir=F:\FTP_DATA\DATA_Mining\cardetect\train-log # 浏览器http://localhost:6006,打不开时使用命令行给定的网址


#<图片缩放>
import matplotlib.pyplot as plt  
import tensorflow as tf  
import numpy as np  
  
image_raw_data = tf.gfile.GFile('C:/Users/DELL/Desktop/CWTPic/train1.jpg','r').read()   #加载原始图像  
  
with tf.Session() as sess:  
    img_data = tf.image.decode_jpeg(image_raw_data)   #解码
    plt.imshow(img_data.eval())  
    plt.show()
    resized = tf.image.resize_images(img_data, [64,64],method=0)  #第一个参数为原始图像，第二个参数为图像大小，第三个参数给出了指定的算法  
    resized = np.asarray(resized.eval(),dtype='uint8')  #变为uint8才能显示
    plt.imshow(resized)   
    plt.show()

<#tf.summary.scalar>
http://cache.baiducontent.com/c?m=9d78d513d98515b8599d837e7a01d6165813df3c6f979b027ea48448e4735a310637b0e574620704a2d20a6216db4c4b9cf12103361767f7c5c7d20c9bf9d33f2efb3a293559c35612a419fe8d1b32c157c107b6b240b3a7f03090a4d0d4c25155ce53067c97f0fc00464b94&p=aa769a47878559ff57e690685153&newp=c271c54addc708b334be9b7c1b4892695d0fc20e3bd3c44324b9d71fd325001c1b69e7bf2d201303d9c37c6406ab4a5deff433763c1766dada9fca458ae7c46133e4&user=baidu&fm=sc&query=tf%2Esummary%2Escalar+%D7%F7%D3%C3&qid=de64187700003764&p1=4

<#屏蔽警报信息>
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
#默认为0：输出所有log信息
#设置为1：进一步屏蔽INFO信息
#设置为2：进一步屏蔽WARNING信息
#设置为3：进一步屏蔽ERROR信息


<报CPU不支持AVX内核错误>
 Advanced Vector Extensions (AVX, also known as Sandy Bridge New Extensions) 先进的矢量扩展（AVX，也称为桑迪桥新的扩展）是从英特尔和英特尔在2008年3月提出的微处理器的X86指令集架构的扩展，第一次由英特尔支持，在第2011季度和以后的SoeBoE桥处理器装运。AMD与推土机处理器航运在Q3 2011。AVX提供了新的特性、新的指令和新的编码方案。AVX2将大多数整数命令扩展为256位，并介绍了融合乘法累加（FMA）操作。AVX-512扩展AVX到512位支持使用一个新的EVEX前缀编码由英特尔提出的2013年7月，第一次支持英特尔与骑士着陆处理器，在2016装运。
 参考网友的评论解释：这个意思其实是，您下载的TensorFlow太low了，根本没有通过兼容AVX来Compile。如果您下载源代码在该电脑上重新compile，就可以支持AVX。其实你的电脑是支持AVX的，只是编译好的TensorFlow不支持。
<batch normalization>
refer: https://www.cnblogs.com/hrlnw/p/7227447.html
1.原理
公式如下：
y=γ(x-μ)/σ+β
其中x是输入，y是输出，μ是均值，σ是方差，γ和β是缩放（scale）、偏移（offset）系数。
一般来讲，这些参数都是基于channel来做的，比如输入x是一个16*32*32*128(NWHC格式)的feature map，那么上述参数都是128维的向量。其中γ和β是可有可无的，有的话，就是一个可以学习的参数（参与前向后向），没有的话，就简化成y=(x-μ)/σ。而μ和σ，在训练的时候，使用的是batch内的统计值，测试/预测的时候，采用的是训练时计算出的滑动平均值。

 

2.tensorflow中使用

tensorflow中batch normalization的实现主要有下面三个：

tf.nn.batch_normalization

tf.layers.batch_normalization

tf.contrib.layers.batch_norm

封装程度逐个递进，建议使用tf.layers.batch_normalization或tf.contrib.layers.batch_norm，因为在tensorflow官网的解释比较详细。我平时多使用tf.layers.batch_normalization，因此下面的步骤都是基于这个。

 

python使用总结：


numpy
a.shape()
a.reshape((2,4)) # reshape新生成数组和原数组公用一个内存，不管改变哪个都会互相影响。



pandas
#< DataFrame >
# 参考 https://www.cnblogs.com/zuizui1204/p/6423069.html
# 未特殊说明series同样适用
a_list_dtype = df.dtypes # 为series
    #DataFrame数据类型
    #1. float
    #2. int
    #3. bool
    #4. datetime64[ns]
    #5. datetime64[ns, tz]
    #6. timedelta[ns]
    #7. category
    #8. object 
np.issubdtype(df['time'][0], np.int)  # 对具体的列或值进行数据类型判断 

df.shape # 查看行列数，df_.shape[0]行数==len(df),df_.shape[1]列数
df.head(3) # 查看3行，tail
df.info()
df.index   # 需要循环迭代显示出来,columns,values类似
df.index.tolist() # 直接转化为list
df.describe()   # 统计汇总，dataframe格式
df.T
df.sort_index(axis = 1, ascending = False, inplace = True) # 按轴排序，axis = 0 是按行排序，1按列名排序，默认升序，FALSE降序，升序（从第一行是1,第二行2）
df.sort(columns = 'time') # 仅对这一列排序
df[df['age'].isin(['2','3'])] # 表显示满足条件：列one中的值包含'2','3'的所有行。
df[df>0] = -df # 赋值

# 缺失值处理，在pandas中，使用np.nan来代替缺失值，这些值将默认不会包含在计算中。
df.fillna(value=x,  inplace = True)   # 用x值填充缺失，注意对原始数据更改时，inplace = True才会更改生效
df.dropna(how='any',  inplace = True) # 表示去掉所有包含缺失值的行

# reindex()方法
# 用来对指定轴上的索引进行改变/增加/删除操作，这将返回原始数据的一个拷贝。即用index=[]表示对index进行操作，columns表对列进行操作。
df.reindex(index=list(df.index)+['five'],columns=list(df.columns)+['d']) # 增加'five'行和'd'列，默认新增数值全为nan
df.reindex(index=['one','five'],columns=list(df.columns)+['d'])

# 合并
# contact 其中list_series表示要进行进行连接的列表数据,axis=1时表横着对数据进行连接。axis=0或不指定时，表将数据竖着进行连接。list_series中要连接的数据有几个则对应几个keys，设置keys是为了在数据连接以后区分每一个原始list_series中的数据。
list_series=[df['WTUR.Other.Wn.I16.year'],df['WTUR.Other.Wn.I16.hour']]
result=pd.concat(list_series,axis=1, keys=['WTUR.Other.Wn.I16.year','WTUR.Other.Wn.I16.hour'])

df.append(df[2:],ignore_index=True) # 表示将a中的第三行以后的数据全部添加到a中，若不指定ignore_index参数，则会把添加的数据的index保留下来，若ignore_index=Ture则会对所有的行重新自动建立索引。

# merge类似于SQL中的join, 设a1,a2为两个dataframe,二者中存在相同的键值，至于四者的具体差别，具体学习参考sql中相应的语法。,两个对象连接的方式有下面几种：
(1)内连接，pd.merge(a1, a2, on='key')
(2)左连接，pd.merge(a1, a2, on='key', how='left')
(3)右连接，pd.merge(a1, a2, on='key', how='right')
(4)外连接，pd.merge(a1, a2, on='key', how='outer')

# 去重
df.drop_duplicates(subset=['A','B'],keep='first',inplace=True) # subset=['A','B']没有时表示这一行所有元素全相同时才去重，df = df.drop_duplicates(inplace =True) 这种用法是错误的，在这被坑过很多次“等号前面一定不能有”


# groupby
pd.date_range('20000101',periods=10) # 用pd.date_range函数生成连续指定天数的的日期
df.groupby('age').sum() # 生成的是dataframe，df.groupby(['age','grade']).sum(),mean(),cumsum()累加量（注意如果是一个list，如[1,2,3,-3]对应的累加值[1,3,6,6]）,min(),size(),字符串类型的列在使用sum时不显示

# Categorical按某一列重新编码分类
df['gender1']=df['gender'].astype('category')    # 新增一列'gender1'，单独此行便能实现series转成category类型，可以节省存储空间。http://liao.cpython.org/pandas15/
df['gender1'].cat.categories=['male','female']  # 即将'gender'中的0，1先转化为category类型再进行编码。

# Reshaping 透视表
# https://www.cnblogs.com/chaosimple/p/4153083.html
stacked = df.stack() # 将列名水平分布调整为竖直分布
stacked.unstack()    # 还原stacked.unstack(0)将第一列索引转换水平排布
pd.pivot_table(df, values='D', index=['index_name1','index_name2'], columns=['C']) # 

# 相关操作
df.index = pd.to_datetime (df[ 'WTUR.Tm.Rw.Dt' ]) # 转换时间形式并增加索引，注意原始时间列会单独保留一列，format="%Y/%m/%d %H:%M"
df.to_csv ('.csv', index=None) # None时不保留索引
df_protocol_need=pd.read_excel('.xlsx')
f = open(filepath) # 有时需要这样
df = pd.read_csv (f,low_memory=False) # 
df.to_excel(r'C:\\Users\\guohuaiqi\\Desktop\\2.xls',sheet_name='Sheet1')    
df=pd.read_excel(r'C:\\Users\\guohuaiqi\\Desktop\\2.xls','Sheet1',na_values=['NA'])
exec('df'+WTGnum+'=df') # 执行字符串语句

# 其他描述性统计：
df.mean() # 默认对每一列的数据求平均值；若加上参数a.mean(1)则对每一行求平均值；
df['x'].value_counts() # 统计某一列x中各个值出现的次数：
df.apply(lambda x:x.max()-x.min()) # 对数据应用函数
df['gender1'].str.lower() # 字符串相关操作 将gender1中所有的英文大写转化为小写，注意dataframe没有str属性，只有series有，所以要选取a中的gender1字段。

# 时间序列
pd.date_range('20000201','20000210',freq='D') # 也可以不指定频数，只指定起始日期。在六中用pd.date_range('xxxx',periods=xx,freq='D/M/Y....')函数生成连续指定天数的的日期列表，'/'表示“或”。其中periods表示持续频数。此外如果不指定freq，则默认从起始日期开始，频率为day。其他频率表示如下：

# pandas画图
# 本身自带的或者使用matplotlib
import matplotlib.pyplot as plt
a=Series(np.random.randn(1000),index=pd.date_range('20100101',periods=1000))
b=a.cumsum()
b.plot()
plt.show()    #最后一定要加这个plt.show()，不然不会显示出图来。






# 数据预处理异常数据筛选 https://blog.csdn.net/sublio/article/details/81327766 《categorical_data(类别型数据)预处理笔记：转化为数字》




python常用模块方法属性强化记忆
#< os >
os.listdir()
os.path.jion([])
glob.glob(r'\*.csv')

#< list >
l.index()

#< zipfile >
z = zipfile.ZipFile (filename, 'r')
z.extract (f.replace ('.arc', '.data'), filepath)
z.close ()

#< sqlite >
import sqlite3 as db
conn = db.connect (dbfile)
cursor = conn.cursor() # 加载全部DB或者ARC数据为DataFrame文件
sql2 = 'select * from realtimedata' # 'realtimedata' 数据库表名               
df = pd.read_sql (sql2, conn)
conn.close() # 关闭数据库




python使用总结： Life is short, You need Python~ 人生苦短，我用python~

# anaconda 快捷命令
Ctrl + Shift + Alt + P打开preference，可以设置一些例如字体，快捷键等。
Ctrl加左键点击变量名称可以跳转到变量定义处。
# %%可以创建cell使代码分部运行，类似于MATLAB，在单元格中Ctrl+enter可以运行当前cell，shift+enter可以运行当前单元格并跳转到下个cell。
Ctrl+P可以打开文件搜索框且支持模糊搜索（fuzzy search），输入@之后可以搜索函数，class名称 。
可视化创建矩阵（Ctrl+M）和向量（Ctrl+Alt+M），快捷键有时和其他软件快捷键冲突
Ctrl+1注释/取消注释行，Ctrl+4/5注释取消注释代码块。
Ctrl+L	清除shell
debug	Ctrl+F5	调试
Ctrl+F12	继续调试（单步）
F5	运行
选中一行，右键可以执行当前行

#pd<常用的统计分析> refer https://www.cnblogs.com/onemorepoint/p/7506099.html 

# Series 常用
S.name # 获取列明



# NaN None "" 三者区别
from numpy import NaN
type(NaN) # float 有时也写成nan，NAN，np.nan,np.NAN 都是float
type(None) # None的类型是Nonetype既‘空对象’,python中没有null类型，但python是把0，空字符串''，空列表[]和None都看作False，把其他数值和非空字符串都看作True
type("") # str # 对象=属性+方法 （其实方法也是一种属性，一种区别于数据属性的可调用属性
# %%
    import pandas as pd
    import numpy as np
    test = {"id":[1,2,3,4,5],
            "birthday":['2000-01-01','',None,'2000-01-19',np.nan],
            "name":['王菲','莫文蔚',np.NAN,None,'林宥嘉'],
            "score":[100,99,np.NaN,98,None]}
    test = pd.DataFrame(test)
    # 结果如下：最后一行pd自己识别是数值，直接将None转换为数值NaN了
         birthday  id  name  score
    0  2000-01-01   1    王菲  100.0
    1               2   莫文蔚   99.0
    2        None   3   NaN    NaN
    3  2000-01-19   4  None   98.0
    4         NaN   5   林宥嘉    NaN
    # 和None NaN的统计相关操作，当进行count操作时，NaN和None都不计算在内，""则被计算在内；当进行sum等计算时，会对除NaN和None以外的值进行计算。
    test['score'].count()
    # 3
    test['score'].mean() # 统计时先看是否是数值类型，同时统计相关操作，默认是不计入NaN的（skip = True）,这和np.nanmean()相同，但是np.mean()是计入NaN的
    # 99.0 # 常用的函数：count 非Na值，min，argmax 最大值索引位置， idxmin 最小值索引值， quantile 分位数， median 算数中位
数， mad var std skew kurt sumsum cummin cumprod diff pct_change   
    test['birthday'].count()
    # 3
    test['score']=test['score'].astype('int') # 因为test的score一列中含有空值NaN，所以无法进行这一项操作，
    test.isnull().any() # all() notnull 判断是否有None NAN ,不能识别空字符"",
    a2 = test.where(test.notnull(), None) # 将NaN替换为None # cond = test.notnull(),other = -df 即满足条件时数值保持不变，否则用other替换。df.mask(cond) 使用时，结果与where相反


numpy
a.shape()
a.reshape((2,4)) # reshape新生成数组和原数组公用一个内存，不管改变哪个都会互相影响。
# %% pd<常用的统计分析>
test = np.array([3,5,4,7,np.nan])
m  = test.mean()   # nan
m1 = np.mean(test) # nan
m2 = np.nanmean(test) # 4.75
    # 注意和哪np区别，如np.mean或者np.nanmean输入的series则两者没有区别，如果输入的np的array两者是有区别的
    aa = np.mean(test['score'])     # 结果：99
    aa1 = np.nanmean(test['score']) # 99

pandas
#< DataFrame >
# 参考 https://www.cnblogs.com/zuizui1204/p/6423069.html
# 未特殊说明series同样适用
a_list_dtype = df.dtypes # 为series
    #DataFrame数据类型
    #1. float
    #2. int
    #3. bool
    #4. datetime64[ns]
    #5. datetime64[ns, tz]
    #6. timedelta[ns]
    #7. category
    #8. object 
np.issubdtype(df['time'][0], np.int)  # 对具体的列或值进行数据类型判断 

df.shape # 查看行列数，df_.shape[0]行数==len(df),df_.shape[1]列数
df.head(3) # 查看3行，tail
df.info()
df.index   # 需要循环迭代显示出来,columns,values类似
df.index.tolist() # 直接转化为list
df.describe()   # 统计汇总，dataframe格式
df.T
df[df['age'].isin(['2','3'])] # 表显示满足条件：列one中的值包含'2','3'的所有行。
df[df>0] = -df # 赋值

# 创建DataFrame
dict = {'MEAN': MEAN,
        'MEDIAN': MEDIAN,
        'MODE': MODE,
        'PTP': PTP,
        'VAR': VAR,
        'STD': STD,
        'CV': CV          
        }
df = pd.DataFrame(dict, index = [data.name]).T
df = pd.DataFrame(data=[[1],[3],[2]], columns=['id','name','sex'])

# 更改列名，索引名
df.columns = ['Id','Name','Sex'] # 重命名
a = a.rename(columns={'A':'a', 'C':'c'}) # refer ： https://www.cnblogs.com/hhh5460/p/5816774.html
c.index=['A','B','C','D','E']

# 常用切片及给切片赋值
df.loc[:,['A']]
df.iloc[:,[13,15]] =df.iloc[:,[13,15]].astype('float')
df[['A']] =df.iloc[:,[13]]
df.loc[df['A']==0] #提取data数据(筛选条件: A列中数字为0所在的行数据)

# 转换数据格式
df.iloc[:,[13,15]] =df.iloc[:,[13,15]].astype('float') #一列中含有空值NaN，所以无法进行这一项操作
df['a'] = pd.to_numeric(df['a'])

# 设置索引
df.set_index(["Column"], inplace=True, drop=False) # drop是否去除这一列

# 排序
df.sort_index(axis = 1, ascending = False, inplace = True) # 按轴排序，sort_index是按索引排序，axis = 0 是按行排序，1按列名排序，默认升序，FALSE降序，升序（从第一行是1,第二行2）
df.sort_index(by='a') # 按这一列或者两列排序 by=['a','c']
df.sort_index(by='C',axis=1) # 按这一行
df.sort_values('time')       # 按这一列排序（df全局变动）


# 查看某列的元素有哪些
list(df['type'].unique())
df['type'].value_counts() # 也可以

# 缺失值处理，在pandas中，使用np.nan来代替缺失值，这些值将默认不会包含在计算中。
df.fillna(value=x,  inplace = True)   # 用x值填充缺失，注意对原始数据更改时，inplace = True才会更改生效
df.fillna({'sex':233,'phone':666})
df.dropna(axis=1,how='any',  inplace = True) # 表示去掉所有包含缺失值的行any或all
test_1 = test.fillna(method='pad',limit = 1) #  用前一个数据替代：
test_2 = test.fillna(method='bfill',limit = 1) # 用后一个数据替代：其中limit代表了允许每一列中多少个NA值被替代~
test_2 = test.fillna(test['score'].mean())  # 用统计量来替代:



# 删除列
df.drop(columns=['B', 'C'])
df.drop(['B', 'C'], axis=1)

# 删除行
df.drop([0, 1], inplace = True)
df.drop(index=[0, 1])



# reindex()方法
# 用来对指定轴上的索引进行改变/增加/删除操作，这将返回原始数据的一个拷贝。即用index=[]表示对index进行操作，columns表对列进行操作。
df.reindex(index=list(df.index)+['five'],columns=list(df.columns)+['d']) # 增加'five'行和'd'列，默认新增数值全为nan
df.reindex(index=['one','five'],columns=list(df.columns)+['d'])

# 合并
# contact 其中list_series表示要进行进行连接的列表数据,axis=1时表横着对数据进行连接。axis=0或不指定时，表将数据竖着进行连接。list_series中要连接的数据有几个则对应几个keys，设置keys是为了在数据连接以后区分每一个原始list_series中的数据。
list_series=[df['WTUR.Other.Wn.I16.year'],df['WTUR.Other.Wn.I16.hour']]
result=pd.concat(list_series,axis=1, keys=['WTUR.Other.Wn.I16.year','WTUR.Other.Wn.I16.hour'])

df.append(df[2:],ignore_index=True) # 表示将a中的第三行以后的数据全部添加到a中，若不指定ignore_index参数，则会把添加的数据的index保留下来，若ignore_index=Ture则会对所有的行重新自动建立索引。

# merge类似于SQL中的join, 设a1,a2为两个dataframe,二者中存在相同的键值，至于四者的具体差别，具体学习参考sql中相应的语法。,两个对象连接的方式有下面几种：
(1)内连接，pd.merge(a1, a2, on='key')
(2)左连接，pd.merge(a1, a2, on='key', how='left')
(3)右连接，pd.merge(a1, a2, on='key', how='right')
(4)外连接，pd.merge(a1, a2, on='key', how='outer')

# 去重
df.drop_duplicates(subset=['A','B'],keep='first',inplace=True) # subset=['A','B']没有时表示这一行所有元素全相同时才去重，df = df.drop_duplicates(inplace =True) 这种用法是错误的，在这被坑过很多次“等号前面一定不能有”


# groupby
pd.date_range('20000101',periods=10) # 用pd.date_range函数生成连续指定天数的的日期
df.groupby('age').sum() # 生成的是dataframe，df.groupby(['age','grade']).sum(),mean(),cumsum()累加量（注意如果是一个list，如[1,2,3,-3]对应的累加值[1,3,6,6]）,min(),size(),字符串类型的列在使用sum时不显示

# Categorical按某一列重新编码分类
df['gender1']=df['gender'].astype('category')    # 新增一列'gender1'，单独此行便能实现series转成category类型，可以节省存储空间。http://liao.cpython.org/pandas15/
df['gender1'].cat.categories=['male','female']  # 即将'gender'中的0，1先转化为category类型再进行编码。

# Reshaping 透视表
# https://www.cnblogs.com/chaosimple/p/4153083.html
stacked = df.stack() # 将列名水平分布调整为竖直分布
stacked.unstack()    # 还原stacked.unstack(0)将第一列索引转换水平排布
pd.pivot_table(df, values='D', index=['index_name1','index_name2'], columns=['C']) # 

# 相关操作
df.index = pd.to_datetime (df[ 'WTUR.Tm.Rw.Dt' ]) # 转换时间形式并增加索引，注意原始时间列会单独保留一列，format="%Y/%m/%d %H:%M"
df.to_csv ('.csv', index=None) # None时不保留索引
df_protocol_need=pd.read_excel('.xlsx')
f = open(filepath) # 有时需要这样
df = pd.read_csv (f,low_memory=False) # 
df.to_excel(r'C:\\Users\\guohuaiqi\\Desktop\\2.xls',sheet_name='Sheet1')    
df=pd.read_excel(r'C:\\Users\\guohuaiqi\\Desktop\\2.xls','Sheet1',na_values=['NA'])
exec('df'+WTGnum+'=df') # 执行字符串语句

# 其他描述性统计：
df.mean() # 默认对每一列的数据求平均值；若加上参数a.mean(1)则对每一行求平均值；
df['x'].value_counts() # 统计某一列x中各个值出现的次数：
df.apply(lambda x:x.max()-x.min()) # 对数据应用函数
df['gender1'].str.lower() # 字符串相关操作 将gender1中所有的英文大写转化为小写，注意dataframe没有str属性，只有series有，所以要选取a中的gender1字段。

# 按周聚合计算平均值（统计分析）
df = pd.pivot_table(df, index=['a'], columns=['b'], aggfunc=numpy.sum)

# 时间序列
pd.date_range('20000201','20000210',freq='D') # 也可以不指定频数，只指定起始日期。在六中用pd.date_range('xxxx',periods=xx,freq='D/M/Y....')函数生成连续指定天数的的日期列表，'/'表示“或”。其中periods表示持续频数。此外如果不指定freq，则默认从起始日期开始，频率为day。其他频率表示如下：

# pandas画图
# 本身自带的或者使用matplotlib
import matplotlib.pyplot as plt
a=Series(np.random.randn(1000),index=pd.date_range('20100101',periods=1000))
b=a.cumsum()
b.plot()
plt.show()    #最后一定要加这个plt.show()，不然不会显示出图来。


# 读取excel
df=pd.read_excel(f, sheet_name,  header = 1, index_col=False) # index_col=False，不设置列索引 # refer # https://blog.csdn.net/brucewong0516/article/details/79096633
    # 或者直接使用xlrd
    import xlrd
    f = r''
    workbook = xlrd.open_workbook(f)
    sheet_names= workbook.sheet_names()
    sheet2 = workbook.sheet_by_name(sheet_names[0])
    rows = sheet2.row_values(0) # 获取第四行内容
    cols = sheet2.col_values(1) # 获取第二列内容

# 读取csv
df=pd.read_csv(filepath_or_buffer, sep=', ', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=None, compact_ints=None, use_unsigned=None, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None)[source]




# 数据预处理异常数据筛选 https://blog.csdn.net/sublio/article/details/81327766 《categorical_data(类别型数据)预处理笔记：转化为数字》

# 数据清洗 http://bluewhale.cc/2016-08-21/python-data-cleaning.html


python常用模块方法属性强化记忆
#< os >
os.listdir()
os.path.jion([])
glob.glob(r'\*.csv')

#< list >
l.index()
l.sort(reverse=True) # list排序reverse = False 升序（默认）

#交集并集差集
x & y # 交集  x 是set，set(list0)
x | y # 并集  
x - y # 差集

#< zipfile >
z = zipfile.ZipFile (filename, 'r')
z.extract (f.replace ('.arc', '.data'), filepath)
z.close ()

#< sqlite >
import sqlite3 as db
conn = db.connect (dbfile)
cursor = conn.cursor() # 加载全部DB或者ARC数据为DataFrame文件
sql2 = 'select * from realtimedata' # 'realtimedata' 数据库表名               
df = pd.read_sql (sql2, conn)
conn.close() # 关闭数据库

#字符串相关操作
str.split(',') # 分割
str.find('hello') # 查找
str.replace('k',' 8') # 替换

#<re> 正则匹配
num = re.findall('\d+',ss) # \d+ 匹配字符串中的数字部分，返回列表

#<三元表达式、列表解析式>
res='aaaaa' if x > y else 'bbbbbbb' 
[i for i in l if i > 20 and i < 50]
